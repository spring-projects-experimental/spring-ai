/*
 * Copyright 2023 - 2024 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.springframework.ai.vertexai.anthropic.model;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.List;

/**
 * @param anthropicVersion The VertexAI mandatory field for calling Anthropic Claude
 * models (default value is 'vertex-2023-10-16').
 * @param messages Input messages.
 * @param system System prompt. A system prompt is a way of providing context and
 * instructions to Claude, such as specifying a particular goal or role. See our
 * <a href="https://docs.anthropic.com/claude/docs/system-prompts">guide</a> to system
 * prompts.
 * @param maxTokens The maximum number of tokens to generate before stopping. Note that
 * our models may stop before reaching this maximum. This parameter only specifies the
 * absolute maximum number of tokens to generate. Different models have different maximum
 * values for this parameter.
 * @param metadata An object describing metadata about the request.
 * @param stopSequences Custom text sequences that will cause the model to stop
 * generating. Our models will normally stop when they have naturally completed their
 * turn, which will result in a response stop_reason of "end_turn". If you want the model
 * to stop generating when it encounters custom strings of text, you can use the
 * stop_sequences parameter. If the model encounters one of the custom sequences, the
 * response stop_reason value will be "stop_sequence" and the response stop_sequence value
 * will contain the matched stop sequence.
 * @param stream Whether to incrementally stream the response using server-sent events.
 * @param temperature Amount of randomness injected into the response.Defaults to 1.0.
 * Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice,
 * and closer to 1.0 for creative and generative tasks. Note that even with temperature of
 * 0.0, the results will not be fully deterministic.
 * @param topP Use nucleus sampling. In nucleus sampling, we compute the cumulative
 * distribution over all the options for each subsequent token in decreasing probability
 * order and cut it off once it reaches a particular probability specified by top_p. You
 * should either alter temperature or top_p, but not both. Recommended for advanced use
 * cases only. You usually only need to use temperature.
 * @param topK Only sample from the top K options for each subsequent token. Used to
 * remove "long tail" low probability responses. Learn more technical details here.
 * Recommended for advanced use cases only. You usually only need to use temperature.
 * @param tools Definitions of tools that the model may use. If provided the model may
 * return tool_use content blocks that represent the model's use of those tools. You can
 * then run those tools using the tool input generated by the model and then optionally
 * return results back to the model using tool_result content blocks.
 * @author Alessio Bertazzo
 * @since 1.0.0
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
public record ChatCompletionRequest(
// @formatter:off
									 @JsonProperty("anthropic_version") String anthropicVersion,
									 @JsonProperty("messages") List<AnthropicMessage> messages,
									 @JsonProperty("system") String system,
									 @JsonProperty("max_tokens") Integer maxTokens,
									 @JsonProperty("metadata") Metadata metadata,
									 @JsonProperty("stop_sequences") List<String> stopSequences,
									 @JsonProperty("stream") Boolean stream,
									 @JsonProperty("temperature") Float temperature,
									 @JsonProperty("top_p") Float topP,
									 @JsonProperty("top_k") Integer topK,
									 @JsonProperty("tools") List<Tool> tools) {
	// @formatter:on

	public ChatCompletionRequest(String anthropicVersion, List<AnthropicMessage> messages, String system,
			Integer maxTokens, Float temperature, Boolean stream) {
		this(anthropicVersion, messages, system, maxTokens, null, null, stream, temperature, null, null, null);
	}

	public ChatCompletionRequest(String anthropicVersion, List<AnthropicMessage> messages, String system,
			Integer maxTokens, List<String> stopSequences, Float temperature, Boolean stream) {
		this(anthropicVersion, messages, system, maxTokens, null, stopSequences, stream, temperature, null, null, null);
	}

	/**
	 * @param userId An external identifier for the user who is associated with the
	 * request. This should be a uuid, hash value, or other opaque identifier. Anthropic
	 * may use this id to help detect abuse. Do not include any identifying information
	 * such as name, email address, or phone number.
	 */
	@JsonInclude(JsonInclude.Include.NON_NULL)
	public record Metadata(@JsonProperty("user_id") String userId) {
	}

	public static ChatCompletionRequestBuilder builder() {
		return new ChatCompletionRequestBuilder();
	}

	public static ChatCompletionRequestBuilder from(ChatCompletionRequest request) {
		return new ChatCompletionRequestBuilder(request);
	}

	public static class ChatCompletionRequestBuilder {

		private String anthropicVersion;

		private List<AnthropicMessage> messages;

		private String system;

		private Integer maxTokens;

		private ChatCompletionRequest.Metadata metadata;

		private List<String> stopSequences;

		private Boolean stream = false;

		private Float temperature;

		private Float topP;

		private Integer topK;

		private List<Tool> tools;

		private ChatCompletionRequestBuilder() {
		}

		private ChatCompletionRequestBuilder(ChatCompletionRequest request) {
			this.anthropicVersion = request.anthropicVersion;
			this.messages = request.messages;
			this.system = request.system;
			this.maxTokens = request.maxTokens;
			this.metadata = request.metadata;
			this.stopSequences = request.stopSequences;
			this.stream = request.stream;
			this.temperature = request.temperature;
			this.topP = request.topP;
			this.topK = request.topK;
			this.tools = request.tools;
		}

		public ChatCompletionRequestBuilder withAnthropicVersion(String anthropicVersion) {
			this.anthropicVersion = anthropicVersion;
			return this;
		}

		public ChatCompletionRequestBuilder withMessages(List<AnthropicMessage> messages) {
			this.messages = messages;
			return this;
		}

		public ChatCompletionRequestBuilder withSystem(String system) {
			this.system = system;
			return this;
		}

		public ChatCompletionRequestBuilder withMaxTokens(Integer maxTokens) {
			this.maxTokens = maxTokens;
			return this;
		}

		public ChatCompletionRequestBuilder withMetadata(ChatCompletionRequest.Metadata metadata) {
			this.metadata = metadata;
			return this;
		}

		public ChatCompletionRequestBuilder withStopSequences(List<String> stopSequences) {
			this.stopSequences = stopSequences;
			return this;
		}

		public ChatCompletionRequestBuilder withStream(Boolean stream) {
			this.stream = stream;
			return this;
		}

		public ChatCompletionRequestBuilder withTemperature(Float temperature) {
			this.temperature = temperature;
			return this;
		}

		public ChatCompletionRequestBuilder withTopP(Float topP) {
			this.topP = topP;
			return this;
		}

		public ChatCompletionRequestBuilder withTopK(Integer topK) {
			this.topK = topK;
			return this;
		}

		public ChatCompletionRequestBuilder withTools(List<Tool> tools) {
			this.tools = tools;
			return this;
		}

		public ChatCompletionRequest build() {
			return new ChatCompletionRequest(anthropicVersion, messages, system, maxTokens, metadata, stopSequences,
					stream, temperature, topP, topK, tools);
		}

	}
}
